---
title: "Binary Response (ELMR Chapter 2)"
author: "Dr. Hua Zhou @ UCLA"
date: "Apr 2, 2020"
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4  
subtitle: Biostat 200C
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', cache = FALSE)
```

Display system information and load `tidyverse` and `faraway` packages
```{r}
sessionInfo()
library(tidyverse)
library(faraway)
```
`faraway` package contains the datasets in the ELMR book.

## Heart disease example

The dataframe `wcgs` in `faraway` package contains data from theWestern Collaborative Group Study.
```{r}
wcgs %>% head(10)
```
We convert the dataframe into a tibble for compatibility with tidyverse.
```{r}
wcgs <- wcgs %>% 
  as_tibble(row_names = "year") %>%
  print(width = Inf)
```

For now, we focus just on variables   
    - `chd`, whether the person develops coronary heard disease or not,  
    - `height`, height of the person in inches,   
    - `cigs`, number of cigarettes smoked per day.  
```{r}
wcgs %>%
  select(chd, height, cigs) %>%
  summary()
```

We use side-by-side boxplots to summarize the qulitative variable `chd` and quantitative variables `height` and `cigs`
```{r}
ggplot(data = wcgs) +
  geom_boxplot(mapping = aes(x = chd, y = height))
```
and number of cigarretes smoked per day 
```{r}
ggplot(data = wcgs) +
  geom_boxplot(mapping = aes(x = chd, y = cigs))
```

It seems more cigarettes is associated with heard disease, but not height.  How can we formally analyze this? If we use linear regression (straight line) for the anlaysis, the line will eventually extends beyond the [0, 1] range, making interpretation hard. 
```{r}
ggplot(data = wcgs) +
  geom_point(mapping = aes(x = cigs, y = chd))
```

## Logistic regression

- Bernoulli model for a binary response
$$
Y_i = \begin{cases}
1 & \text{with probability } p_i \\
0 & \text{with probability } 1 - p_i
\end{cases}
$$

- The parameter $p_i = \mathbb{E}(Y_i)$ will be related to the predictors $X_1, \ldots, X_{q}$ via an **inverse link function**
$$
p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}},
$$
where $\eta_i$ is the **linear predictor** or **systematic component**
$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{q} x_{iq} = \mathbf{x}_i^T \boldsymbol{\beta}
$$
with
$$
\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_q \end{pmatrix}, \quad \mathbf{x}_i = \begin{pmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{iq} \end{pmatrix}.
$$

- The function
$$
\eta = g(p) = \log \left( \frac{p}{1-p} \right)
$$
that links $\mathbb{E}(Y)$ to the systematic component is called the **link function**. This particular link function is also called the **logit function**.

- The function 
$$
p = g^{-1}(\eta) = \frac{e^\eta}{1 + e^\eta}
$$
is called the **inverse link function**. This particular function (inverse logit) is also called the **logistic function**. A graph of the logistic function:
```{r}
ggplot(data = data.frame(x = 0), mapping = aes(x = x)) + # null data
  stat_function(fun = ilogit) + # ilogit is from faraway
  xlim(-6, 6) + 
  labs(x = expression(eta), y = "p")
# curve(ilogit(x), -6, 6, xlab = expression(eta), ylab = "p")
```

- Therefore above model is called the **logistic regression**.  

## Fitting logistic regression

- We use **method of maximum likelihood** (MLE) to estimate the parameters $\beta_0, \ldots, \beta_q$. Given $n$ data points $(y_i, \mathbf{x}_i)$, $i=1,\ldots,n$, the log-likelihood is
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_i \log \left[p_i^{y_i} (1 - p_i)^{1 - y_i}\right] \\
&=& y_i \log p_i + (1 - y_i) \log (1 - p_i) \\
&=& y_i \log \frac{e^{\eta_i}}{1 + e^{\eta_i}} + (1 - y_i) \log \frac{1}{1 + e^{\eta_i}} \\
&=& y_i \eta_i - \log (1 + e^{\eta_i}) \\
&=& y_i \cdot \mathbf{x}_i^T \boldsymbol{\beta} - \log (1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}}).
\end{eqnarray*}
HW1: show that the log-likelihood function of logistic regression is a concave function in $\boldsymbol{\beta}$. If you need a refresher how to take derivatives with respect to a vector or matrix, see [Biostat 216 notes](https://ucla-biostat216-2019fall.github.io/slides/16-matrixcalc/16-matrixcalc.html).

    Maximization of this log-likehood function can be carried out by the Newton-Raphson algorithm.
```{r}
(lmod <- glm(chd ~ height + cigs, family = binomial, wcgs))
```
Inspect the content in the result `lmod`:
```{r}
str(lmod)
```
Summary of the result:
```{r}
(lmod_sm <- summary(lmod))
str(lmod_sm)
```

## Interpretation

- **Exercise**: Before we attempt to interpret the results from logistic regression, we first need to understand how the data is transformed to $(y_i, \mathbf{x}_i)$.
```{r}
# dataframe
wcgs %>%
  select(chd, height, cigs) %>%
  head(10)
# response
lmod$y %>% head(10)
# predictors
model.matrix(lmod) %>% head(10)
```

- How to interpret the regression coefficients in logistic regression? Remember
$$
\log \left( \frac{p}{1-p} \right) = \beta_0 + \beta_1 \cdot \text{height} + \beta_2 \cdot \text{cigs}. 
$$
The quantity
$$
o = \frac{p}{1-p}
$$
is called **odds** (of an event). 

    Therefore $\beta_1$ can be interpreted as a unit increase in $x_1$ with $x_2$ held fixed increases the **log-odds** of success by $\beta_1$, or increase the odds of success by a factor of $e^{\beta_1}$. 
- **Exercise**: Interpret the regression coefficients from `wcgs` fit.  
```{r}
# same as lmod$coefficients
# coef(lmod) is a named numeric vector
(beta_hat <- unname(coef(lmod)))
exp(beta_hat)
```
How to interpret the effect of a pack a day (20 cigarettes) on heart disease?
```{r}
exp(beta_hat[3] * 20)
```

- Suppose the probability of success in the presence of some condition is $p_1$ and $p_2$ in its absence. The **relative risk** is $p_1 / p_2$. For example, the predicted probability of a 68in tall person who smokes a pack (20 cigarettes) a day and who does not smoke are, respectively
```{r}
(p1 <- ilogit(sum(beta_hat * c(1, 68, 20))))
```
and
```{r}
(p2 <- ilogit(sum(beta_hat * c(1, 68, 0))))
```
Then the relative risk is
```{r}
p1 / p2
```

## Inference (analysis of deviance)

- The **deviance** of a logistic regression fit is 
$$
D = -2 \sum_i \left[ \widehat{p}_i \text{logit}(\widehat{p}_i) + \log (1 - \widehat{p}_i) \right].
$$
It comes from the likelihood ratio test (LRT) statistic 
$$
2 \log \frac{L_{\Omega}}{L_{\omega}},
$$
where $\Omega$ is the full/saturated model (same number of parameters as observations) and $\omega$ is the smaller model. 

- In the model output, the _residual deviance_, denoted $D_L$, is the devience of the current model and the _null deviance_, denoted $D_S$, is the deviance of the model with just an intercept term. Assuming the null model, the test statistic $D_S - D_L$ is asymptotically distributed $\chi_{\ell - s}^2$. In our case, the test statistic is
```{r}
lmod$null.deviance - lmod$deviance
```
giving p-value
```{r}
1 - pchisq(lmod$null.deviance - lmod$deviance, 2)
```
Therefore our model gives a significantly better fit than the null (intercept-only) model.

- We can also test the significance of individual predictor using analysis of deviance (`anova` function). For example, is `height` necessary in the model?
```{r}
# fit a model without height
lmodc <- glm(chd ~ cigs, family = binomial, wcgs)
anova(lmodc, lmod, test = "Chi")
```

- Similar to linear regression, the convenience function `drop1` tests each individual predictor in one shot.
```{r}
drop1(lmod, test = "Chi")
```

- The coefficient test from summary is based on the z-value $\hat \beta_j / \text{se}(\hat{\beta}_j)$.
```{r}
lmod_sm$coefficients
```
In general, deviance-based test is preferred over the z-test. 

## Confidence intervals

- Confidence interval can be constructed either from normal approximation
$$
\hat \beta_j \pm z^{\alpha / 2} \text{se}(\hat \beta_j)
$$
```{r}
tibble(
  `coef`  = beta_hat,
  `2.5%`  = beta_hat - 1.96 * lmod_sm$coefficients[, 2],
  `97.5%` = beta_hat + 1.96 * lmod_sm$coefficients[, 2])
```
or from profile-likelihood
```{r}
confint(lmod)
```

## Diagnosis

- There are two kinds of fitted values (or predicted values). The first is on the scale of the linear predictor, $\eta$,
```{r}
linpred  <- predict(lmod)
linpred %>% head(10)
```
The second on the scale of response, $p = \text{logit}^{-1}(\eta)$,
```{r}
predprob <- predict(lmod, type = "response")
predprob %>% head(10)
```

- We compute the **raw residuals**
$$
y - \widehat{p}
$$
```{r}
# same as residuals(lmod, type = "response")
rawres <- lmod$y - predprob
```
The plot of raw residuals against the fitted values is not very informative.
```{r}
wcgs %>%
  mutate(rawres = rawres, linpred = linpred) %>%
  ggplot() +
  geom_point(mapping = aes(x = linpred, y = rawres)) +
  labs(x = "linear predictor", y = "raw residuals")
```
We do not expect the raw residuals to have equal variance because the binary variance is $p(1 - p)$.

- The **deviance residuals** are standardized residuals defined by
$$
d_i = \text{sign}(y_i - \widehat{p}_i) \sqrt{-2 [y_i \log\widehat{p}_i + (1 - y_i) \log(1 - \widehat{p}_i)]}.
$$
Note 
$$
\sum_i d_i^2 = \text{deviance}
$$
in analogy to $\sum_i \widehat{\epsilon}_i^2 = \text{RSS}$ in linear regression. The term $\text{sign}(y_i - \widehat{p}_i)$ ensures that $d_i$ has the same sign as raw residual $y_i - \widehat{p}_i$.
```{r}
devres <- residuals(lmod)
devres %>% head(10)
```
Sanity check:
```{r}
sqrt(-2 * (lmod$y * log(predprob) + (1 - lmod$y) * log(1 - predprob))) %>%
  head(10)
```
The plot of deviance residuals against the fitted values.
```{r}
wcgs %>%
  mutate(devres = devres, linpred = linpred) %>%
  ggplot() +
  geom_point(mapping = aes(x = linpred, y = devres)) +
  labs(x = "linear predictor", y = "deviance residuals")
```
Again we see the residuals are clustered into two lines: the upper one corresponding to $y_i=1$ and the lower one to $y_i=0$. We can improve this plot by binning: divide the range of linear predictor into 100 bins of roughly equal points and plot average residual against average linear predictors per bin. 
```{r}
wcgs %>%
  mutate(devres = devres, linpred = linpred) %>% 
  group_by(cut(linpred, breaks = unique(quantile(linpred, (1:100)/101)))) %>%
  summarize(devres = mean(devres), 
            linpred = mean(linpred)) %>%
  ggplot() +
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "linear predictor", y = "deviance residual")
```
**Question**: is there a concern that the deviance residuals are not centered around 0? 

- **Exercise**: Do similar binned plots for deviance residuals against `weights` and deviance residuals vs `cigs`.

- QQ plot is not helpful since there is no reason these deviance residuals are approximately standard normal.
```{r}
qqnorm(devres)
```
Half-normal plot can help detect unusual cases in predictor space.
```{r}
halfnorm(hatvalues(lmod))
```
We see two outliers, who smoke an unusual number of cigarettes per day!
```{r}
wcgs %>%
  slice(c(2527, 2695)) %>%
  print(width = Inf)
```

## Model selection

- We can perform sequential search using the Akaike information criterion (AIC)
$$
\text{AIC} = \text{deviance} + 2q.
$$
We start from a rich enough model
```{r}
wcgs <- wcgs %>%
  # 1 in = 0.0254 m, 1 lb = 0.4536 kg
  mutate(bmi = 703 * weight / height)
biglm <-glm(chd ~ age + height + weight + bmi + sdp + dbp + chol + dibep + cigs + arcus, 
            family = binomial, data = wcgs)
```
and then do sequential search using the `step` function
```{r}
smalllm <- step(biglm, trace = TRUE)
summary(smalllm)
```

## Cross validation

Logistic regression is frequently used for classification purpose. Cross validation provides one way to evaluate model. We will use `caret` package to compare the two logistic regression fits `bigglm` and `smallglm`. TODO