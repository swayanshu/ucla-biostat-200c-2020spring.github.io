---
title: "Count Regression (ELMR Chapter 5)"
author: "Dr. Hua Zhou @ UCLA"
date: "Apr 16, 2020"
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4  
subtitle: Biostat 200C
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', cache = FALSE)
```

Display system information and load `tidyverse` and `faraway` packages
```{r}
sessionInfo()
library(tidyverse)
library(faraway)
```
`faraway` package contains the datasets in the ELMR book.

## Galápagos data

The `gala` data set records the number of plant species and the number of endemic species  for 30 Galápagos islands. We are interested in modeling the number of species using a regression.  

![](./galap_position.png)
![](./galap.png)
![](./Tartarugas-gigantes-de-Galápagos.jpg)
```{r}
gala <- as_tibble(gala, row_names = "Island") %>%
  print(n = Inf)
```

## Poisson regession

- Assume the count response $Y_i$ follows a Poisson$(\mu_i)$ distribution with density function
$$
\mathbb{P}(Y_i = y_i) = e^{-\mu_i} \frac{\mu_i^{y_i}}{y_i!}.
$$

- The Poisson parameter $\mu_i$ is related to the predictors via an **inverse link function**
$$
\mu_i = e^{\eta_i},
$$
where $\eta_i$ is the **linear predictor** or **systematic component**
$$
\eta_i = \mathbf{x}_i^T \boldsymbol{\beta}.
$$

- The function
$$
\eta_i = g(\mu_i) = \log \mu_i
$$
that links $\mathbb{E} Y_i = \mu_i$ to the systematic component is called the **link function**. This particular link function is called the **log link function**.  

- Given the $n$ data points $(y_i, \mathbf{x}_i)$, $i=1,\ldots,n$, the log-likelihood is
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_i y_i \log \mu_i - \mu_i - \log y_i! \\
&=& \sum_i y_i \cdot \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i!
\end{eqnarray*}

- We maximize the log-likelihood function to find the MLE of regression coefficient $\boldsymbol{\beta}$. 

```{r}
modp <- glm(Species ~ . - Endemics, family = poisson, data = gala)
summary(modp)
```

## Interpretation

```{r}
library(gtsummary)
modp %>%
  tbl_regression(intercept = TRUE)
```

## Goodness of fit

- The deviance for the Poisson regression is
\begin{eqnarray*}
  D &=& 2 \sum_i [y_i \log(y_i) - y_i] - 2 \sum_i [y_i \log (\widehat{\mu}_i) - \widehat{\mu}_i] \\
  &=& 2 \sum_i y_i \log(y_i / \widehat{\mu}_i) - (y_i - \widehat{\mu}_i),
\end{eqnarray*}
where $\widehat{\mu}_i$ are the fitted values from the model. The Poisson deviance is also called the **G-statistic**. 

- Comparing the deviance $D$ to $\chi_{n - p}^2$ gives an extremely small p-value (why?), indicating a lack of fit. 

- An alternative goodness of fit test compares the **Pearson $X^2$**
$$
X^2 = \sum_i \frac{(O_i - E_i)^2}{E_i} = \sum_i \frac{(y_i - \widehat{\mu}_i)^2}{\widehat{\mu}_i}
$$
to the asymptotic distribution $\chi_{n - p}^2$. Again it indicates serious lack of fit.
```{r}
# predmu = predict(modp, type = "response")
# sum((gala$Species - predmu)^2 / predmu)
(px2 <- sum(residuals(modp, type = "pearson")^2))
```

## Diagnostics

- Plot deviance residuals against linear predictor. We don't see outliers.
```{r}
gala %>%
  mutate(devres  = residuals(modp, type = "deviance"), 
         linpred = predict(modp, type = "link")) %>%
  ggplot + 
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "Linear predictor", y = "Deviance residual")
```

- Plot Pearson residuals against linear predictor. We don't see outliers.
```{r}
gala %>%
  mutate(perres  = residuals(modp, type = "pearson"),
         linpred = predict(modp, type = "link")) %>%
  ggplot + 
  geom_point(mapping = aes(x = linpred, y = perres)) + 
  labs(x = "Linear predictor", y = "Pearson residual")
```

- For Poisson distribution, $\operatorname{Var}Y_i = \mathbb{E}Y_i = \mu_i$. Let's check this assumption. Looks like we have overdispersion.
```{r}
gala %>%
  mutate(predmu = predict(modp, type = "response"), 
         res    = Species - predmu) %>%
  ggplot() +
  geom_point(mapping = aes(x = predmu, y = res^2)) +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = expression(hat(mu)), y = expression((y - hat(mu))^2)) + 
  scale_x_log10() + 
  scale_y_log10()
```

## Overdispersion

- In Overdispersion model, we assume an **overdispersion parameter** $\phi$ such that
$$
\operatorname{Var} Y_i = \phi \mu_i.
$$

- Given MLE, the overdispersion parameter is estimated by
$$
\widehat{\phi} = \frac{X^2}{n - p}.
$$
```{r}
(dp <- px2 / modp$df.residual)
```

- The inference on regression coefficient is changed with overdispersion parameter
```{r}
summary(modp, dispersion = dp)
```
```{r}
drop1(modp, scale = dp, test = "Chisq")
```

## Quasi-Poisson

- Quasi-Poisson assumes
$$
\mathbb{E}Y_i = \mu_i, \quad \operatorname{Var}Y_i = \phi \mu_i.
$$

```{r}
modd <- glm(Species ~ . - Endemics, family = quasipoisson, data = gala)
summary(modd)
```
```{r}
drop1(modd, test = "F")
```

## Negative bionomial regression

- An alternative to Poisson model is the negative binomial distribution
$$
\mathbb{P}(Y = y) = \binom{y + r - 1}{r - 1} (1 - p)^r p^y, \quad y = 0, 1, \ldots
$$
HW2: show $\mathbb{E}Y = \mu = rp / (1 - p)$ and $\operatorname{Var} Y = r p / (1 - p)^2$.

![](./Negbinomial.gif)

- We can link mean response $\mu$ to the predictors via log link function
$$
\eta = \mathbf{x}^T \boldsymbol{\beta} = \log \mu.
$$

- Pre-specify `r` and fit negative bionomial regression:
```{r}
library(MASS)
modn <- glm(Species ~ . - Endemics, family = negative.binomial(1), data = gala)
summary(modn)
```

- Estimate parameter `r` from data:
```{r}
glm.nb(Species ~ . - Endemics, data = gala) %>%
  summary()
```

## Zero-inflated count models

- The `bioChemists` data set contains 915 biochemistry graduate students. The outcome of interest is the number of articles procuded during the last three years of the PhD.
```{r}
library(pscl)

(bioChemists <- as_tibble(bioChemists))
```

- Descriptive statistics.
```{r}
bioChemists %>%
  ggplot() + 
  geom_bar(mapping = aes(x = art, fill = fem))
```

```{r}
bioChemists %>%
  ggplot() + 
  geom_bar(mapping = aes(x = art, fill = mar))
```
